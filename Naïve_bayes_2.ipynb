{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
        "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
        "probability that an employee is a smoker given that he/she uses the health insurance plan?"
      ],
      "metadata": {
        "id": "Jf3xSIjyhfk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the probability that an employee is a smoker given that they use the health insurance plan, you can use conditional probability. You want to find \\(P(\\text{Smoker} | \\text{Uses Insurance Plan})\\).\n",
        "\n",
        "You are given the following information:\n",
        "\n",
        "1. \\(P(\\text{Uses Insurance Plan}) = 0.70\\) (70% of employees use the plan).\n",
        "2. \\(P(\\text{Smoker} | \\text{Uses Insurance Plan}) = 0.40\\) (40% of employees who use the plan are smokers).\n",
        "\n",
        "You can use the formula for conditional probability:\n",
        "\n",
        "\\[P(\\text{A | B}) = \\frac{P(\\text{A and B})}{P(\\text{B})}\\]\n",
        "\n",
        "In this case:\n",
        "- A represents \"Smoker,\" and B represents \"Uses Insurance Plan.\"\n",
        "\n",
        "So, you want to find \\(P(\\text{Smoker} | \\text{Uses Insurance Plan})\\).\n",
        "\n",
        "Plugging in the values:\n",
        "\n",
        "\\[P(\\text{Smoker} | \\text{Uses Insurance Plan}) = \\frac{P(\\text{Smoker and Uses Insurance Plan})}{P(\\text{Uses Insurance Plan})}\\]\n",
        "\n",
        "You are given that \\(P(\\text{Uses Insurance Plan}) = 0.70\\) and \\(P(\\text{Smoker | Uses Insurance Plan}) = 0.40\\).\n",
        "\n",
        "Now, you can calculate \\(P(\\text{Smoker and Uses Insurance Plan})\\):\n",
        "\n",
        "\\[P(\\text{Smoker and Uses Insurance Plan}) = P(\\text{Smoker | Uses Insurance Plan}) \\cdot P(\\text{Uses Insurance Plan}) = 0.40 \\cdot 0.70 = 0.28\\]\n",
        "\n",
        "Finally, use this value to find \\(P(\\text{Smoker | Uses Insurance Plan})\\):\n",
        "\n",
        "\\[P(\\text{Smoker | Uses Insurance Plan}) = \\frac{0.28}{0.70} = \\frac{4}{10} = 0.4\\]\n",
        "\n",
        "So, the probability that an employee is a smoker, given that they use the health insurance plan, is 0.4 or 40%."
      ],
      "metadata": {
        "id": "F6iAOL5dhkqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
      ],
      "metadata": {
        "id": "J67HoTb1hpX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm, and they are used for different types of data and have different assumptions. Here's a summary of the key differences between Bernoulli Naive Bayes and Multinomial Naive Bayes:\n",
        "\n",
        "1. Data Type:\n",
        "   - Bernoulli Naive Bayes: It is used for binary or binary-like data, where features represent the presence (1) or absence (0) of specific attributes. In this model, the focus is on whether a feature is present or not.\n",
        "   - Multinomial Naive Bayes: It is used for discrete data, often applied to text data where features represent counts or frequencies of words or terms in a document. The features are non-negative integers.\n",
        "\n",
        "2. Feature Representation:\n",
        "   - Bernoulli Naive Bayes: Each feature is treated as a binary variable, typically indicating whether a term or word is present in a document (1) or not (0).\n",
        "   - Multinomial Naive Bayes: Features are typically represented as counts (non-negative integers) or frequencies (ratios) of occurrences of different categories (e.g., word counts in a document).\n",
        "\n",
        "3. Assumptions:\n",
        "   - Bernoulli Naive Bayes: Assumes that the features are conditionally independent given the class label. It focuses on the presence or absence of features and models how they are associated with class labels.\n",
        "   - Multinomial Naive Bayes: Assumes that the features follow a multinomial distribution. It models the distribution of feature counts (or frequencies) within documents for different classes.\n",
        "\n",
        "4. Use Cases:\n",
        "   - Bernoulli Naive Bayes: Suitable for text classification tasks, such as spam detection, where the primary concern is whether specific words or terms are present in a document.\n",
        "   - Multinomial Naive Bayes: Commonly used for text classification and document categorization tasks, where the focus is on the frequency or count of words in documents, making it more suitable for tasks like sentiment analysis or topic modeling.\n",
        "\n",
        "5. Binary Features:\n",
        "   - Bernoulli Naive Bayes: Assumes binary features (present or absent). Often used for binary text classification tasks.\n",
        "   - Multinomial Naive Bayes: Handles non-negative integer features (word counts or frequencies), allowing for more expressive text representations.\n",
        "\n",
        "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of your data and the assumptions that align with your problem. If your features are binary, representing presence or absence, and you are dealing with binary classification tasks, Bernoulli Naive Bayes may be more appropriate. If your features are discrete counts or frequencies, especially in text classification problems, Multinomial Naive Bayes is a better choice."
      ],
      "metadata": {
        "id": "8unBoGvFhqM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does Bernoulli Naive Bayes handle missing values?"
      ],
      "metadata": {
        "id": "J5R45gkdhv4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm, and they are used for different types of data and have different assumptions. Here's a summary of the key differences between Bernoulli Naive Bayes and Multinomial Naive Bayes:\n",
        "\n",
        "1. Data Type:\n",
        "   - Bernoulli Naive Bayes: It is used for binary or binary-like data, where features represent the presence (1) or absence (0) of specific attributes. In this model, the focus is on whether a feature is present or not.\n",
        "   - Multinomial Naive Bayes: It is used for discrete data, often applied to text data where features represent counts or frequencies of words or terms in a document. The features are non-negative integers.\n",
        "\n",
        "2. Feature Representation:\n",
        "   - Bernoulli Naive Bayes: Each feature is treated as a binary variable, typically indicating whether a term or word is present in a document (1) or not (0).\n",
        "   - Multinomial Naive Bayes: Features are typically represented as counts (non-negative integers) or frequencies (ratios) of occurrences of different categories (e.g., word counts in a document).\n",
        "\n",
        "3. Assumptions:\n",
        "   - Bernoulli Naive Bayes: Assumes that the features are conditionally independent given the class label. It focuses on the presence or absence of features and models how they are associated with class labels.\n",
        "   - Multinomial Naive Bayes: Assumes that the features follow a multinomial distribution. It models the distribution of feature counts (or frequencies) within documents for different classes.\n",
        "\n",
        "4. Use Cases:\n",
        "   - Bernoulli Naive Bayes: Suitable for text classification tasks, such as spam detection, where the primary concern is whether specific words or terms are present in a document.\n",
        "   - Multinomial Naive Bayes: Commonly used for text classification and document categorization tasks, where the focus is on the frequency or count of words in documents, making it more suitable for tasks like sentiment analysis or topic modeling.\n",
        "\n",
        "5. Binary Features:\n",
        "   - Bernoulli Naive Bayes: Assumes binary features (present or absent). Often used for binary text classification tasks.\n",
        "   - Multinomial Naive Bayes: Handles non-negative integer features (word counts or frequencies), allowing for more expressive text representations.\n",
        "\n",
        "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of your data and the assumptions that align with your problem. If your features are binary, representing presence or absence, and you are dealing with binary classification tasks, Bernoulli Naive Bayes may be more appropriate. If your features are discrete counts or frequencies, especially in text classification problems, Multinomial Naive Bayes is a better choice."
      ],
      "metadata": {
        "id": "r3FBR9vCh0xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
      ],
      "metadata": {
        "id": "XvNdbvOdh34U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is one of the variants of the Naive Bayes algorithm, and it's particularly well-suited for handling continuous or real-valued data. It's often used for both binary classification and multi-class classification problems. Here's how it works for multi-class classification:\n",
        "\n",
        "In the context of multi-class classification, Gaussian Naive Bayes works by modeling the likelihood of each feature for each class as a Gaussian (normal) distribution. It assumes that the features are continuous and that their values are sampled from Gaussian distributions with different means and variances for each class. When you want to classify a new data point into one of the multiple classes, the algorithm calculates the conditional probability of the data point belonging to each class using Bayes' theorem and then assigns the class with the highest probability as the predicted class.\n",
        "\n",
        "To perform multi-class classification with Gaussian Naive Bayes:\n",
        "\n",
        "1. Model the Data: Estimate the mean and variance of the features for each class in your training dataset.\n",
        "\n",
        "2. Calculate the Class Priors: Determine the prior probability of each class in the training data.\n",
        "\n",
        "3. For a new data point, calculate the conditional probability of the features given each class using Gaussian probability density functions.\n",
        "\n",
        "4. Use Bayes' theorem to calculate the posterior probability of each class given the data point.\n",
        "\n",
        "5. Assign the class with the highest posterior probability as the predicted class for the data point.\n",
        "\n",
        "Gaussian Naive Bayes is often used in situations where the assumption of normally distributed features is reasonable, and it can work well for multi-class classification when applied to real-valued data. However, it may not perform as effectively as some other algorithms, particularly when the assumptions of the Gaussian distribution do not hold, or when the data is highly skewed or has a complex relationship with the class labels. In such cases, other machine learning algorithms or variations of Naive Bayes, such as Multinomial Naive Bayes or Bernoulli Naive Bayes, may be more appropriate for multi-class classification tasks."
      ],
      "metadata": {
        "id": "oxBo_0Vgh_Uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Assignment:\n",
        "Data preparation:\n",
        "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
        "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
        "is spam or not based on several input features.\n",
        "Implementation:\n",
        "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
        "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
        "dataset. You should use the default hyperparameters for each classifier.\n",
        "Results:\n",
        "Report the following performance metrics for each classifier:\n",
        "Accuracy\n",
        "Precision\n",
        "Recall\n",
        "F1 score\n",
        "Discussion:\n",
        "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
        "the case? Are there any limitations of Naive Bayes that you observed?\n",
        "Conclusion:\n",
        "Summarise your findings and provide some suggestions for future work."
      ],
      "metadata": {
        "id": "2l8GHWNDiIWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can guide you through the process of implementing and evaluating Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python, as well as reporting the performance metrics. Here's a step-by-step guide to help you get started:\n",
        "\n",
        "**Step 1: Download and Prepare the Data**\n",
        "\n",
        "1. Download the \"Spambase Data Set\" from the UCI Machine Learning Repository: [Spambase Data Set](https://archive.ics.uci.edu/ml/datasets/Spambase).\n",
        "\n",
        "2. Load the dataset into a Pandas DataFrame and prepare it for machine learning.\n",
        "\n",
        "**Step 2: Import Libraries and Load the Data**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"spambase.data\", header=None)\n",
        "```\n",
        "\n",
        "**Step 3: Prepare the Data**\n",
        "\n",
        "Preprocess the data, including separating features and labels, splitting the data into training and testing sets (if needed), and any other necessary data preparation steps.\n",
        "\n",
        "**Step 4: Implement and Evaluate Naive Bayes Classifiers**\n",
        "\n",
        "```python\n",
        "# Split the data into features (X) and labels (y)\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Initialize classifiers\n",
        "bernoulli_nb = BernoulliNB()\n",
        "multinomial_nb = MultinomialNB()\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# Perform 10-fold cross-validation for each classifier\n",
        "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
        "scores_bernoulli = cross_val_score(bernoulli_nb, X, y, cv=10, scoring=scoring)\n",
        "scores_multinomial = cross_val_score(multinomial_nb, X, y, cv=10, scoring=scoring)\n",
        "scores_gaussian = cross_val_score(gaussian_nb, X, y, cv=10, scoring=scoring)\n",
        "```\n",
        "\n",
        "**Step 5: Report Performance Metrics**\n",
        "\n",
        "```python\n",
        "# Calculate mean scores for each metric\n",
        "mean_scores_bernoulli = {metric: scores.mean() for metric, scores in zip(scoring, scores_bernoulli.T)}\n",
        "mean_scores_multinomial = {metric: scores.mean() for metric, scores in zip(scoring, scores_multinomial.T)}\n",
        "mean_scores_gaussian = {metric: scores.mean() for metric, scores in zip(scoring, scores_gaussian.T)}\n",
        "\n",
        "# Print the results\n",
        "print(\"Bernoulli Naive Bayes:\")\n",
        "print(mean_scores_bernoulli)\n",
        "print(\"\\nMultinomial Naive Bayes:\")\n",
        "print(mean_scores_multinomial)\n",
        "print(\"\\nGaussian Naive Bayes:\")\n",
        "print(mean_scores_gaussian)\n",
        "```\n",
        "\n",
        "**Step 6: Discussion**\n",
        "\n",
        "Discuss the results you obtained, including which variant of Naive Bayes performed the best and why you think that is the case. You can also mention any limitations or insights you observed during the evaluation.\n",
        "\n",
        "**Step 7: Conclusion**\n",
        "\n",
        "Summarize your findings and provide suggestions for future work or further analysis.\n",
        "\n",
        "Make sure you have the `scikit-learn` library installed in your Python environment. You can install it using pip:\n",
        "\n",
        "```bash\n",
        "pip install scikit-learn\n",
        "```\n",
        "\n",
        "This code framework allows you to load the Spambase dataset, implement the three Naive Bayes classifiers, and evaluate their performance using 10-fold cross-validation while reporting key metrics. You can adapt and expand on this code as needed for your analysis."
      ],
      "metadata": {
        "id": "VQTqTVfRiOIV"
      }
    }
  ]
}